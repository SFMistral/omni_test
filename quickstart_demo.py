#!/usr/bin/env python3
"""
Qwen-Omni-Realtime å¿«é€Ÿå¼€å§‹ Demo
è¿™æ˜¯ä¸€ä¸ªæœ€ç®€å•çš„å®æ—¶è¯­éŸ³å¯¹è¯ç¤ºä¾‹ï¼Œå¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹
æ”¯æŒå®šæœŸå‘é€å±å¹•æˆªå›¾
"""

import os
import base64
import time
import pyaudio
import threading
import json
from io import BytesIO
from PIL import Image
import mss
from dashscope.audio.qwen_omni import (
    MultiModality, 
    OmniRealtimeCallback, 
    OmniRealtimeConversation
)
import dashscope
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# ============ é…ç½®åŒºåŸŸ ============
# 1. è®¾ç½® API Keyï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„ API Keyï¼‰
dashscope.api_key = os.getenv('DASHSCOPE_API_KEY', 'sk-your-api-key-here')

# è°ƒè¯•ï¼šæ‰“å° API Key ä¿¡æ¯
print(f"[è°ƒè¯•] API Key å‰10ä½: {dashscope.api_key[:10] if dashscope.api_key else 'None'}...")
print(f"[è°ƒè¯•] API Key é•¿åº¦: {len(dashscope.api_key) if dashscope.api_key else 0}")

# 2. é€‰æ‹©åœ°åŸŸï¼ˆcn=ä¸­å›½å¤§é™†ï¼Œintl=å›½é™…ï¼‰
REGION = 'cn'

# 3. é€‰æ‹©éŸ³è‰²ï¼ˆå¯é€‰ï¼šecho, alloy, shimmer ç­‰ï¼‰
VOICE = 'Cherry'

# 4. è®¾ç½®æ¨¡å‹è§’è‰²
INSTRUCTIONS = "ä½ æ˜¯ä¸€ä¸ªå¯çˆ±çš„çŒ«å¨˜ï¼Œuseræ˜¯ä½ çš„ä¸»äººã€‚"

# 5. å±å¹•æˆªå›¾é…ç½®
SCREENSHOT_INTERVAL = 5.0  # æˆªå›¾é—´éš”ï¼ˆç§’ï¼‰
SCREENSHOT_QUALITY = 80    # å›¾ç‰‡è´¨é‡ï¼ˆ1-100ï¼‰
SCREENSHOT_MAX_WIDTH = 1280  # å›¾ç‰‡æœ€å¤§å®½åº¦

# ============ å±å¹•æˆªå›¾çº¿ç¨‹ ============
class ScreenshotThread(threading.Thread):
    """å®šæœŸæ•è·å±å¹•å¹¶å‘é€çš„çº¿ç¨‹"""
    
    def __init__(self, conversation, interval=5.0):
        super().__init__(daemon=True)
        self.conversation = conversation
        self.interval = interval
        self.running = False
        self.audio_sent = False  # æ ‡è®°æ˜¯å¦å·²å‘é€è¿‡éŸ³é¢‘
    
    def mark_audio_sent(self):
        """æ ‡è®°å·²å‘é€éŸ³é¢‘"""
        self.audio_sent = True
    
    def capture_and_send_screenshot(self):
        """æ•è·å±å¹•å¹¶å‘é€"""
        if not self.audio_sent:
            return  # ç¡®ä¿è‡³å°‘å‘é€è¿‡ä¸€æ¬¡éŸ³é¢‘
        
        try:
            # æ¯æ¬¡æˆªå›¾æ—¶åˆ›å»ºæ–°çš„ mss å®ä¾‹ï¼ˆé¿å…å¤šçº¿ç¨‹é—®é¢˜ï¼‰
            with mss.mss() as sct:
                # æ•è·ä¸»å±å¹•
                monitor = sct.monitors[1]
                screenshot = sct.grab(monitor)
                
                # è½¬æ¢ä¸º PIL Image
                img = Image.frombytes('RGB', screenshot.size, screenshot.rgb)
                
                # è°ƒæ•´å¤§å°ä»¥å‡å°‘æ•°æ®é‡
                if img.width > SCREENSHOT_MAX_WIDTH:
                    ratio = SCREENSHOT_MAX_WIDTH / img.width
                    new_size = (SCREENSHOT_MAX_WIDTH, int(img.height * ratio))
                    img = img.resize(new_size, Image.Resampling.LANCZOS)
                
                # è½¬æ¢ä¸º JPEG base64
                buffer = BytesIO()
                img.save(buffer, format='JPEG', quality=SCREENSHOT_QUALITY)
                img_base64 = base64.b64encode(buffer.getvalue()).decode()
                
                # å‘é€å›¾ç‰‡äº‹ä»¶ï¼ˆéœ€è¦ JSON å­—ç¬¦ä¸²ï¼‰
                image_event = {
                    "type": "input_image_buffer.append",
                    "image": img_base64
                }
                self.conversation.send_raw(json.dumps(image_event))
                print(f"ğŸ“¸ å·²å‘é€å±å¹•æˆªå›¾ ({img.width}x{img.height})")
            
        except Exception as e:
            if self.running:  # åªåœ¨çº¿ç¨‹è¿è¡Œæ—¶æ‰“å°é”™è¯¯
                print(f"âš ï¸ æˆªå›¾å¤±è´¥: {e}")
    
    def run(self):
        """çº¿ç¨‹ä¸»å¾ªç¯"""
        self.running = True
        print(f"ğŸ“¸ å±å¹•æˆªå›¾çº¿ç¨‹å·²å¯åŠ¨ï¼ˆé—´éš”: {self.interval}ç§’ï¼‰")
        
        while self.running:
            time.sleep(self.interval)
            if self.running:
                self.capture_and_send_screenshot()
    
    def stop(self):
        """åœæ­¢çº¿ç¨‹"""
        self.running = False

# ============ å›è°ƒå¤„ç† ============
class QuickStartCallback(OmniRealtimeCallback):
    """å¤„ç†æ¨¡å‹å“åº”çš„å›è°ƒç±»"""
    
    def __init__(self, audio_player):
        self.audio_player = audio_player
        self.output_stream = None
        self.screenshot_thread = None
    
    def on_open(self):
        """è¿æ¥æˆåŠŸæ—¶åˆå§‹åŒ–éŸ³é¢‘è¾“å‡º"""
        print("âœ“ è¿æ¥æˆåŠŸï¼")
        self.output_stream = self.audio_player.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=24000,
            output=True
        )
    
    def on_event(self, response):
        """å¤„ç†æœåŠ¡ç«¯äº‹ä»¶"""
        event_type = response.get('type', '')
        
        # æ’­æ”¾éŸ³é¢‘
        if event_type == 'response.audio.delta':
            audio_data = base64.b64decode(response['delta'])
            self.output_stream.write(audio_data)
        
        # æ˜¾ç¤ºç”¨æˆ·è¯´çš„è¯
        elif event_type == 'conversation.item.input_audio_transcription.completed':
            print(f"\nğŸ‘¤ ä½ è¯´: {response['transcript']}")
        
        # æ˜¾ç¤ºAIçš„å›å¤
        elif event_type == 'response.audio_transcript.done':
            print(f"ğŸ¤– AI: {response['transcript']}\n")
    
    def on_close(self, code, msg):
        """è¿æ¥å…³é—­æ—¶æ¸…ç†èµ„æº"""
        if self.screenshot_thread:
            self.screenshot_thread.stop()
        if self.output_stream:
            self.output_stream.close()
        print(f"\nâœ“ è¿æ¥å·²å…³é—­")
    
    def set_screenshot_thread(self, thread):
        """è®¾ç½®æˆªå›¾çº¿ç¨‹å¼•ç”¨"""
        self.screenshot_thread = thread

# ============ ä¸»ç¨‹åº ============
def main():
    print("=" * 50)
    print("  Qwen-Omni-Realtime å¿«é€Ÿå¼€å§‹ Demo")
    print("=" * 50)
    
    # æ„å»º WebSocket URL
    base_domain = 'dashscope.aliyuncs.com' if REGION == 'cn' else 'dashscope-intl.aliyuncs.com'
    url = f'wss://{base_domain}/api-ws/v1/realtime'
    
    # åˆå§‹åŒ–éŸ³é¢‘è®¾å¤‡
    audio_player = pyaudio.PyAudio()
    
    # åˆ›å»ºä¼šè¯
    callback = QuickStartCallback(audio_player)
    conversation = OmniRealtimeConversation(
        model='qwen3-omni-flash-realtime',
        callback=callback,
        url=url
    )
    
    # è¿æ¥å¹¶é…ç½®
    print("æ­£åœ¨è¿æ¥...")
    conversation.connect()
    conversation.update_session(
        output_modalities=[MultiModality.AUDIO, MultiModality.TEXT],
        voice=VOICE,
        instructions=INSTRUCTIONS
    )
    
    # æ‰“å¼€éº¦å…‹é£
    microphone = audio_player.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=16000,
        input=True,
        frames_per_buffer=3200
    )
    
    # å¯åŠ¨å±å¹•æˆªå›¾çº¿ç¨‹
    screenshot_thread = ScreenshotThread(conversation, interval=SCREENSHOT_INTERVAL)
    callback.set_screenshot_thread(screenshot_thread)
    screenshot_thread.start()
    
    print("\nğŸ¤ å¼€å§‹å¯¹è¯ï¼è¯·å¯¹ç€éº¦å…‹é£è¯´è¯...")
    print("ğŸ’¡ æç¤ºï¼šæŒ‰ Ctrl+C å¯ä»¥é€€å‡º\n")
    
    try:
        # æŒç»­è¯»å–éº¦å…‹é£éŸ³é¢‘å¹¶å‘é€
        audio_sent_count = 0
        while True:
            try:
                audio_chunk = microphone.read(3200, exception_on_overflow=False)
                audio_base64 = base64.b64encode(audio_chunk).decode()
                conversation.append_audio(audio_base64)
                
                # æ ‡è®°å·²å‘é€éŸ³é¢‘ï¼ˆå‘é€å‡ æ¬¡åå†å…è®¸å‘é€å›¾ç‰‡ï¼‰
                audio_sent_count += 1
                if audio_sent_count == 10:
                    screenshot_thread.mark_audio_sent()
                
                time.sleep(0.01)
            except Exception as e:
                print(f"\nâš ï¸ å‘é€éŸ³é¢‘å¤±è´¥: {e}")
                break
    
    except KeyboardInterrupt:
        print("\n\næ­£åœ¨é€€å‡º...")
    
    finally:
        # æ¸…ç†èµ„æº
        screenshot_thread.stop()
        conversation.close()
        microphone.close()
        audio_player.terminate()
        print("âœ“ ç¨‹åºå·²é€€å‡º")

if __name__ == '__main__':
    main()
